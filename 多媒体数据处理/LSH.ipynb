{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38308cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68040, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot, floor, argsort\n",
    "from numpy.random import random, uniform\n",
    "import numba as nb\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 导入数据集\n",
    "data = np.loadtxt('corel', usecols=range(1, 33))\n",
    "# 加载预先算好的前1000个点的真实10近邻\n",
    "true_idxes = pd.read_csv('true_idxes.csv', header=None)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf27249",
   "metadata": {},
   "source": [
    "&emsp;&emsp;欧氏距离对应的LSH Hash Function为：$H(v) =  \\lceil\\frac{v·r + b}{a}\\rceil$，$r$是一个随机向量，$a$是桶宽，$b$是一个在$[0,a]$之间均匀分布的随机变量。$v·r$可以看做是将$v$向$r$上进行投影操作。$H(v)$是(a/2,2a,1/2,1/3)-sensitive的。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370f671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算欧氏距离\n",
    "# 不用numba加速，用范数计算快，但用了numba，普通计算更快\n",
    "@nb.jit(nopython=True)\n",
    "def calc_dist(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2)) \n",
    "    # return np.linalg.norm(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd67ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: 32x桶数, b: 1x桶数, a: 标量\n",
    "# 将向量映射到各桶的索引\n",
    "def hash_and_fill(inputs, R, b, a):\n",
    "    # 初始化空的hash_table\n",
    "    buckets = [{} for _ in range(bucket_num)]\n",
    "    mapped_idxes = floor((dot(inputs, R) + b) / a) # 68040x10，每一行是这个点在所有桶中的哈希值\n",
    "    for i, hash_keys in enumerate(mapped_idxes):\n",
    "        for j, hash_key in enumerate(hash_keys):\n",
    "            # 每个桶是一个字典，其中的所有key对应该桶的所有索引键值，每个key对应的value是一个list，里面存放映射到该桶、该索引键值的所有点在原数据集的idx\n",
    "            buckets[j].setdefault(hash_key, []).append(i)\n",
    "    return buckets  # [ {8: [0, 2, 5,...], 12: [2, 12, 16,...] }, {}, {}, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14af28",
   "metadata": {},
   "source": [
    "##### 并集法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e91a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询与q相似的向量，并输出相似度最高的k个向量在原数据集中的索引\n",
    "def find(q, k, R, b, a, buckets):\n",
    "    hash_keys = np.floor((dot(q, R) + b) / a)[0]   # 不加[0]返回的是1xtables_num的矩阵，取[0]转为数组\n",
    "    # 遍历q点的索引键列表，找各桶中与其索引键值相等的点\n",
    "    for i, hash_key in enumerate(hash_keys):\n",
    "        if i == 0:\n",
    "            candi_set = set(buckets[0][hash_key])\n",
    "        else:\n",
    "            candi_set = candi_set.union(buckets[i][hash_key])  # 候选集：在各桶中的索引与q相同的点取并集（若取交集，会导致候选集元素个数接近1，不现实）\n",
    "    candi_set = list(candi_set)    # 转为list便于遍历，因为'set' object does not support indexing\n",
    "    # 遍历候选集，求出离q最近的k个点并返回\n",
    "    dist = [calc_dist(data[i], q) for i in candi_set]\n",
    "    set_idxes = argsort(dist)[1: k + 1]  # set_idxes是近邻点在候选集中的索引，要将其转为在原数据集中的索引\n",
    "    res = [candi_set[i] for i in set_idxes]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2688b6",
   "metadata": {},
   "source": [
    "##### 交集法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询与q相似的向量，并输出相似度最高的k个向量在原数据集中的索引\n",
    "def find2(q, k, R, b, a, buckets):\n",
    "    hash_val = np.floor((dot(q, R) + b) / a)[0]   # 不加[0]返回的是1xtables_num的矩阵，取[0]转为数组\n",
    "    # 遍历q点的索引键列表，找各桶中与其索引键值相等的点\n",
    "    for i, idx_key in enumerate(hash_val):\n",
    "        if i == 0:\n",
    "            candi_set = set(buckets[0][idx_key])\n",
    "        else:\n",
    "            candi_set = candi_set.intersection(buckets[i][idx_key])  # 候选集：在各桶中的索引与q相同的点取并集（若取交集，会导致候选集元素个数接近1，不现实）\n",
    "    candi_set = list(candi_set)    # 转为list便于遍历，因为'set' object does not support indexing\n",
    "    # 遍历候选集，求出离q最近的k个点并返回\n",
    "    dist = [calc_dist(data[i], q) for i in candi_set]\n",
    "    set_idxes = argsort(dist)[1: k + 1]  # set_idxes是近邻点在候选集中的索引，要将其转为在原数据集中的索引\n",
    "    res = [candi_set[i] for i in set_idxes]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96079c0",
   "metadata": {},
   "source": [
    "##### 测试一下单点的检索效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：0.0591秒\n",
      "[33344, 64186, 51051, 38838, 46277, 61134, 56634, 52696, 64170, 38844]\n",
      "[33344, 64186, 51051, 38838, 46277, 61134, 56634, 52696, 64170, 38844]\n"
     ]
    }
   ],
   "source": [
    "# tables_num: hash_table的个数\n",
    "# d: 向量的维数\n",
    "# a是桶宽。a越大，被纳入同个位置的向量就越多，即可以提高原来相似的向量映射到同个位置的概率，反之，则可以降低原来不相似的向量映射到同个位置的概率。因此a需要权衡\n",
    "bucket_num = 15   # 3\n",
    "R = random([32, bucket_num])\n",
    "a = 0.05   # 0.05\n",
    "b = uniform(0, a, [1, bucket_num])\n",
    "\n",
    "\n",
    "p = 0\n",
    "# 进行哈希映射，求出各桶中各索引值下有哪些点\n",
    "buckets = hash_and_fill(data, R, b, a)\n",
    "\n",
    "tic = time.clock()\n",
    "res = find(data[p], 10, R, b, a, buckets)\n",
    "toc = time.clock()\n",
    "print('耗时：{:.4f}秒'.format(toc - tic))\n",
    "\n",
    "# 与暴搜的ground_truth对比\n",
    "print(res)\n",
    "dist_real = [calc_dist(data[i], data[p]) for i in range(data.shape[0])]\n",
    "print(argsort(dist_real)[1 : 11].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c2050",
   "metadata": {},
   "source": [
    "##### 求前1000点的10近邻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beaf7660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "耗时：68.0579秒\n"
     ]
    }
   ],
   "source": [
    "# LSH\n",
    "pred_idx = []\n",
    "search_lens = []\n",
    "tic = time.clock()\n",
    "for i in range(1000):\n",
    "    res = find(data[i], 10, R, b, a, buckets)\n",
    "    pred_idx.append(res)\n",
    "toc = time.clock()\n",
    "print('耗时：{:.4f}秒'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab68744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些实验结果记录：\n",
    "# a = 0.05, m = 5,  30s,  95.50%\n",
    "# a = 0.03, m = 5,  40s,  90.11%\n",
    "# a = 0.01, m = 5,  10s,  50.96%\n",
    "\n",
    "# a = 0.05, m = 5,  30s,  95.50%\n",
    "# a = 0.05, m = 10, 53s,  99.74%\n",
    "# a = 0.05, m = 15, 69s,  99.98%\n",
    "\n",
    "# a = 0.05, m = 3,  24s,  88.33%\n",
    "# a = 0.07, m = 3,  36s,  95.10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a610198",
   "metadata": {},
   "source": [
    "##### 计算前1000点的检索precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efee6738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查准率：99.9700%\n",
      "召回率：99.9700%\n",
      "准确率：100.0000%\n"
     ]
    }
   ],
   "source": [
    "# 计算准确率\n",
    "def count_true_num(pred, true):\n",
    "    a = [i in true for i in pred]\n",
    "    return sum(a)\n",
    "\n",
    "n = 1000\n",
    "precisions = []\n",
    "recalls = []\n",
    "accuracys = []\n",
    "for i in range(n):\n",
    "    TP = count_true_num(pred_idx[i], true_idxes.iloc[i,:].to_list())\n",
    "    FP = 10 - TP\n",
    "    FN = 10 - TP\n",
    "    TN = 68040 - 10 - FP\n",
    "    precisions.append(TP / (TP + FP))\n",
    "    recalls.append(TP / (TP + FN))\n",
    "    accuracys.append((TP + TN) / (TP + TN + FP + FN))\n",
    "\n",
    "p, r, acc = np.mean(precisions), np.mean(recalls), np.mean(accuracys)\n",
    "print(\"查准率：{:.4f}%\\n召回率：{:.4f}%\\n准确率：{:.4f}%\".format(p*100, r*100, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd678c1",
   "metadata": {},
   "source": [
    "##### 下面是预先算好true_label，存入csv文件，方便求p, r, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预先算好true_label，存入csv文件\n",
    "tic = time.clock()\n",
    "true_idxes = []\n",
    "dists = []\n",
    "for i in range(1000):\n",
    "    dist_real = [calc_dist(ColorHist[j], ColorHist[i]) for j in range(ColorHist.shape[0])]\n",
    "    true_idx = np.argsort(dist_real)[1 : 11].tolist()\n",
    "    true_idxes.append(true_idx)\n",
    "    dists.append(dist_real)\n",
    "toc = time.clock()\n",
    "print('耗时：{:.4f}秒'.format(toc - tic))    # 497s\n",
    "df1 = pd.DataFrame(true_idxes)\n",
    "df2 = pd.DataFrame(dists)\n",
    "df1.to_csv('true_idxes.csv', index = False, header = False)\n",
    "df2.to_csv('dists.csv', index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9862ae77e9daaaf9c9239620ed827aad4ce184b3776eb7a3f75df899d88e405b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('gluon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
